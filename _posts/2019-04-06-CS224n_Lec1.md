---
layout:     post
title:      Stanford - CS224n, NLP with Deep learning
subtitle:   Winter 2019
date:       2019-04-03
author:     Zhejian Peng
header-img: img/stanford_logo3.jpg
catalog: true
tags:
    - NLP
    - CS224n
    - Stanford
---

<!-- Add math equation API -->

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

# Lecture 1
[Course Website](http://web.stanford.edu/class/cs224n/)

[Lecture Video](http://onlinehub.stanford.edu/cs224)

[Lecture slids](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf)

#### What do we hope to learn?
- RNN

- Attention

- PyTorch

- Word meaning, dependency parsing, machine translation, question answering

 

Traditional NLP use **one-hot** encoding for words.
The problem is that the one-hot matrix can be very big.
There is no natural notion of similarity of one-hot vectors because they are orthogonal.

We can use a word similarity tabl, yet the table will be very large (500,000 words X 500,000 words)!

#### Denotational Semantics VS. Distributional Semantics

- Denotational Semantics: signifier (symbol) ‚ü∫ signified (idea or thing)

- Distributional Semantics: A word's meaning is given by the words that frequently appear close-by


#### Word2vec
Idea:

1. We have a large corpus of text

2. Every word in a fixed vocabulary is represented by a **vector**

3. Go through each position t in the text, which has a center word c and context ("outside") words o

4. Use the **similarity of the word vectors** for c and o to calculate the probability of o given c (or vice versa)

5. **Keep adjusting the word vectors** to maximize this probability

Example windows and process for computing $${P(w_{t+t} | w_t)}.$$
![-](https://raw.githubusercontent.com/JazzikPeng/jazzikpeng.github.io/master/img/CS224n_Stanford/SS1.png)

We want to maximize the overall log likelihood of getting a correct prediction of context words given center word. The **objective function** is:

$$ Likelihood = L(\theta)=\prod_{t=1}^{T} \prod_{-m \leq j \leq m \atop j \neq 0} P\left(w_{t+j} | w_{t} ; \theta\right) $$

Then the **objective function** is the average negative log likelihood.

$$ J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right) $$

